---
title: "A data-driven planning framework for curbside loading zones - exploratory data analysis"
author: " "
date: 2024-01-30
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)
```


## 1 Introduction

Sam


## 2 Data Manipulation and Visualization





```{r , include=FALSE}

# You can set some global options for knitting chunks

knitr::opts_chunk$set(echo = TRUE)

# Load some libraries

library(tidyverse)
library(tidycensus)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(broom)
library(tufte)
library(rmarkdown)
library(hexbin)
library(viridis)
library(cbsodataR)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(stargazer)
library(jsonlite)
library(ggplot2)
library(tmap)
library(tmaptools)
library(leaflet)
library(raster)
library(RColorBrewer)
library(mapview)
library(leaflet)
library(plotly)
library(ggspatial)
library(openxlsx)
library(lubridate)
library(dplyr)
library(tidyr)
library(reshape2)
library(riem)

#library(rjson)

# functions and data directory

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette2 <- c('#3E4A89','#1F9E89')
palette4 <- c('#3E4A89','#1F9E89','#35B779','#B4DE2C')
palette5 <- c('#440154','#3E4A89','#1F9E89','#35B779','#B4DE2C')
palette6 <- c('#440154','#3E4A89','#1F9E89','#35B779','#B4DE2C','#FDE725')
palette10 <- c('#440154','#482777','#3E4A89','#31688E','#26828E','#1F9E89','#35B779','#6DCD59','#B4DE2C','#FDE725')

```

```{r setting working directory}
#Shengqian
#setwd("/Users/sqwang/Library/CloudStorage/OneDrive-PennO365/penn/6th/Practicum/Data")

#Sam
#setwd("")

#michael
#setwd("")

#tiffany
#setwd("")

#ling
setwd("/Users/lingchen/Documents/Practicum/Practicum_Philly_2024/Data")


coords_raw <- read.csv("./Old_Data_for_Testing/zoneCoords.csv")
# split coords
coords_split <- strsplit(coords_raw$coords, ",")
coords_raw$latitude <- as.numeric(sapply(coords_split, `[`, 1))
coords_raw$longitude <- as.numeric(sapply(coords_split, `[`, 2))
# create coords shapefile
coords <- st_as_sf(coords_raw, coords = c("longitude", "latitude"), crs = 'ESRI:102729')


linAssets <- read_sf('./resmartloadingzones/Curb_and_Asset_Shapefiles/linear_assets.shp')%>% st_transform('ESRI:102729')
# load booking data
booking_data <-st_read("./Old_Data_for_Testing/Booking Data_cleaned.geojson")

# load curb zone data
curb_zone <-  read_json("./resmartloadingzones/curb_zones.json")

# load CDS event data
event_export <- read.csv("./events_export_CDS.csv")

# load regulation geojson
regulations <- st_read("./resmartloadingzones/regulations.geojson")

# load pilot data
#pre_pilot <- read.xlsx("./PHL_Pre-Pilot NN Collection Data_2022_12.xlsx")
#mid_pilot <- read.xlsx("./PHL_Mid-Pilot NN Collection_2023_02.xlsx")

# OMF Data
OMF <- st_read("./zones_with_OMF_vars.geojson")

```

### Old Data for Testing - Booking Data Exploratory Analysis
#### Utilization of curb zones
```{r}
utilization <- booking_data %>%
  dplyr::select(SmartZoneName,OMFCurbZoneID,EventType,ViolationType,VehicleType,TimeRequestedHour,Amount,geometry)

utilization$OMFCurbZoneID <- substr(utilization$OMFCurbZoneID, nchar(utilization$OMFCurbZoneID)-2, nchar(utilization$OMFCurbZoneID))

curb_geom<- utilization %>%
  select(SmartZoneName,OMFCurbZoneID,geometry) %>%
  group_by(SmartZoneName,OMFCurbZoneID) %>%
  summarise(Events=n()) 

```

```{r timesegment}
utilization$DwellTimeMinutes <- utilization$TimeRequestedHour * 60

utilization$TimeSegment <- cut(utilization$DwellTimeMinutes,
                        breaks = c(0, 5, 15, 30, 45, 61),
                        labels = c("0-5", "6-15", "16-30", "31-45","46-60"),
                        right = FALSE)

utilization <- utilization %>%
  mutate(TimeSegment = as.character(TimeSegment), # Convert factor to character
         TimeSegment = replace_na(TimeSegment, "violation")) # Replace NA with "violation"

utilization$TimeSegment <- factor(utilization$TimeSegment)

utilization <- utilization %>%
  mutate(ViolationType = ifelse(ViolationType == "", "booking", ViolationType)) 

```

```{r}
# dwell time - min
grid.arrange(
ggplot(utilization, aes(x = DwellTimeMinutes)) + geom_histogram(bins = 20, fill = "#440154", color = "white") +
  labs(title = "Dwell Time Distribution", x = "Dwell Time (minutes)", y = "Frequency")+
  theme_minimal(),

ggplot(utilization, aes(x = TimeSegment)) +
  geom_bar(fill = "#440154", color = "white") + 
  theme_minimal() + 
  labs(title = "Counts by Time Segment", x = "Time Segment", y = "Count") + 
  theme_minimal(),

# vehicle type:commercial/medium commercial/other/truck
ggplot(utilization, aes(x = VehicleType)) + geom_bar(fill = "#440154") +
  labs(title = "Vehicle Type Distribution", x = "Vehicle Type", y = "Count")+
  theme_minimal(),

# vehicle type: car/freight/truck/van
ggplot(utilization, aes(x = EventType)) + geom_bar(fill = "#440154") +
  labs(title = "Event Type Distribution", x = "Event Type", y = "Count")+
  theme_minimal(),
nrow=2)

```

```{r}
# Availability of curb zones
# vehicle count by curb zone and tiem segment
availability_analysis <- utilization %>%
  group_by(OMFCurbZoneID, TimeSegment) %>%
  summarise(VehicleCount = n())

ggplot(availability_analysis, aes(x = OMFCurbZoneID, y = VehicleCount, fill = TimeSegment)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = palette6)+
  labs(title = "Vehicle Count by Curb Zone and Time Segment", x = "Curb Zone ID", y = "Vehicle Count")

# violation type distribution by curb zone
violation_analysis <- utilization %>%
  group_by(OMFCurbZoneID, ViolationType) %>%
  summarise(Count = n())

ggplot(violation_analysis, aes(x = OMFCurbZoneID, y = Count, fill = ViolationType)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = palette6)+
  labs(title = "Violation Type Distribution by Curb Zone", x = "Curb Zone ID", y = "Count")

```

```{r eval=FALSE, include=FALSE}
utilization_sf <- st_as_sf(utilization, wkt = "geometry", crs = 4326)

# distribution of Curb Zones
ggplot() +
  annotation_map_tile(zoom = 12) +  # Adjust the zoom level as needed
  geom_sf(data = utilization_sf, aes(color = TimeSegment), size = 0.7) +
  scale_fill_manual(values = palette6)+
  labs(title = "Spatial Distribution of Curb Zones in Philadelphia", x = "Longitude", y = "Latitude") +
  theme_minimal()

```

### Event Data Exploratory Analysis
```{r id_clean, include=FALSE}
event_export$curb_zone_id <- substr(event_export$curb_zone_id, nchar(event_export$curb_zone_id)-2, nchar(event_export$curb_zone_id))

event_export$event_session_id <- substr(event_export$event_session_id, nchar(event_export$event_session_id)-3, nchar(event_export$event_session_id))
```

```{r time_clean, echo=TRUE}
# Correctly convert the timestamps from milliseconds to POSIXct
event_export$event_time_start = as.POSIXct(event_export$event_time_start / 1000, origin="1970-01-01", tz="UTC")
event_export$event_time_end = as.POSIXct(event_export$event_time_end / 1000, origin="1970-01-01", tz="UTC")

# Calculate dwell time in seconds
event_export$dwell_time = difftime(event_export$event_time_end, event_export$event_time_start)

# Convert the dwell time to minutes
event_export$dwell_time_minutes = as.numeric(event_export$dwell_time, units = "mins")
event_export$dwell_time_minutes <- round(event_export$dwell_time_minutes,2)

# Add time series data
event_export$event_time_start <- as.POSIXct(event_export$event_time_start, format = "%Y-%m-%d %H:%M:%S")
event_export <- event_export%>%
  mutate(Date=as.Date(event_time_start),
         week=week(Date),
         dotw=wday(Date,label=TRUE),
         interval60=floor_date(event_time_start,unit ="hour"),
         interval15=floor_date(event_time_start,unit="15 mins"),
         time_of_day=case_when(hour(interval60)<7 | hour(interval60)>19 ~ "overnight",
                               hour(interval60)>=7 & hour(interval60)<10 ~ "AM_Rush",
                               hour(interval60)>=10 & hour(interval60)<15 ~ "Mid_Day",
                               hour(interval60)>=15 & hour(interval60)<=19 ~ "PM_Rush"),
         weekend=ifelse(dotw %in% c('Sun','Sat'),'Weekend','Weekday'))

head(event_export)
```

```{r exploratory_analysis_curb}
#curb utilization rates
utilization_rates <- event_export %>%
  group_by(curb_zone_id) %>%
  summarise(Total_Events = n(),
            Total_Dwell_Time = sum(dwell_time_minutes),
            Average_Dwell_Time = mean(dwell_time_minutes))

plot1 <- ggplot(utilization_rates,aes(x=curb_zone_id,y=Total_Events))+
  geom_bar(stat="identity",fill="#3E4A89")+
  theme_minimal()+
  labs(x="Curb Zone ID", y = "Total Events", title="Total Events by Curb Zone")
plot2 <- ggplot(utilization_rates,aes(x=curb_zone_id,y=Total_Dwell_Time))+
  geom_bar(stat="identity",fill="#1F9E89")+
  theme_minimal()+
  labs(x="Curb Zone ID", y = "Total Events", title="Total Dwell Time by Curb Zone")
plot3 <- ggplot(utilization_rates,aes(x=curb_zone_id,y=Average_Dwell_Time))+
  geom_bar(stat="identity",fill="#35B779")+
  theme_minimal()+
  labs(x="Curb Zone ID", y = "Average Dwell Time", title="Average Dwell Time by Curb Zone")

grid.arrange(plot1,plot2,plot3,nrow=3)
```

```{r exploratory_serial}
#peak parking times
event_export$event_time_start <- as.POSIXct(event_export$event_time_start, format = "%Y-%m-%d %H:%M:%S")
peak_times <- event_export%>%
  mutate(Hour = format(event_time_start, "%H")) %>%
  group_by(Hour) %>%
  summarise(Events = n())

#event patterns over time
patterns <- event_export %>%
  group_by(Date=as.Date(event_time_start)) %>%
  summarise(Events=n()) %>%
  mutate(week=week(Date),
         dotw=wday(Date,label=TRUE))

byhour <- event_export %>%
  group_by(time_of_day) %>%
  summarise(Events=n())

byweekend<- event_export %>%
  group_by(weekend) %>%
  summarise(Events=n())

weekend_hour <- event_export %>%
  group_by(weekend, hour(interval60)) %>%
  summarize(Events=n()) %>%
  rename(hour='hour(interval60)')

ggplot(peak_times,aes(x=Hour,y=Events))+
  geom_bar(stat="identity",fill="#1F9E89")+
  theme_minimal()+
  labs(x="Hour", y = "Events", title="Events by Hour of the Day")

  ggplot(byweekend, aes(weekend,Events)) + geom_bar(stat="identity",fill="#1F9E89") + 
    labs(title="Event Patterns by Weekend vs Weekday", x="Weekend or Weekday", y="Event Counts") + theme_minimal()
  

  grid.arrange(
  ggplot(patterns, aes(Date,Events)) + geom_line(color="#1F9E89") + 
    labs(title="Event Patterns by Time, from Oct 2022 to Apr 2023", x="Date", y="Event Counts") + theme_minimal(),
   ggplot(patterns, aes(dotw,Events)) + geom_bar(stat="identity",fill="#1F9E89") + 
    labs(title="Event Patterns by Day of the Week, from Oct 2022 to Apr 2023", x="Date", y="Event Counts") + theme_minimal(),
  
  ggplot(byhour, aes(time_of_day,Events)) + geom_bar(stat="identity",fill="#1F9E89") + 
    labs(title="Event Patterns by Hour of the Day", x="Hour", y="Event Counts") + theme_minimal(),

  ggplot(weekend_hour, aes(x=hour, y=Events, color=weekend)) + geom_line() + 
    scale_color_manual(values=palette2)+
    labs(title="Event Patterns under Week and Time", x="Hour", y="Event Counts") + theme_minimal(),
  nrow=2)
   
  #Correlation
  correlation <- cor(event_export$dwell_time_minutes,event_export$vehicle_length)
```

```{r exploratory_operation}
#parking duration distribution

#vehicle type analysis
vehicle_type <- event_export %>%
  group_by(vehicle_type) %>%
  summarise(Total_Events=n())

grid.arrange(
ggplot(event_export, aes(x=dwell_time_minutes)) +
  geom_histogram(binwidth = 5,fill="#FDE725",color="white") +
  theme_minimal()+
  labs(x="Dwell Time (minutes)", y="Count", title="Parking Duration Distribution"),
ggplot(vehicle_type,aes(x=vehicle_type,y=Total_Events))+
  geom_bar(stat="identity",fill="#FDE725")+
  theme_minimal()+
  labs(x="Vehicle Type", y = "Events", title="Events by Vehicle Type")+
  theme_minimal(),
nrow=2)
```

### OpenStreetMap exploration

The "**amenity**" types we collected were: "bar", "cafe", "fast_food", "pub", "restaurant", "college", "school", "university", "parking_space", "bank", "atm", "clinic", "hospital", "pharmacy", "community_centre", "conference_centre", "nightclub", "theatre", "police", "post_box", "post_office", "place_of_worship"

The "**building**" types we collected were: "apartments", "dormitory", "hotel", "commercial", "office", "retail", "supermarket", "warehouse", "church", "college", "government", "hospital", "public", "school", "university"

The "**shop**" types we collected were: "alcohol", "bakery", "beverages", "coffee", "convenience", "deli", "department_store", "general", "supermarket", "clothes", "gift"

We also collected **office** building types and **land uses** but these data were not a large enough sample.

```{r openstreemap location collection, message=FALSE}
# amenities <- c("bar", "cafe", "fast_food", "pub", "restaurant", "college", "school", "university", 
#                    "parking_space", "bank", "atm", "clinic", "hospital", "pharmacy", "community_centre", 
#                    "conference_centre", "nightclub", "theatre", "police", "post_box", "post_office", 
#                    "place_of_worship")
# 
# # Format the list: remove underscores, capitalize first letter, and put in quotes
# formatted_list <- gsub("_", " ", amenities)
# formatted_list <- gsub("^([a-z])", "\\U\\1", formatted_list, perl = TRUE)
# formatted_list_1 <- paste0('"', formatted_list, '"')
# 
# buildings <- c("apartments", "dormitory", "hotel", "commercial", "office", 
#                    "retail", "supermarket", "warehouse", "church", "college", 
#                    "government", "hospital", "public", "school", "university")
# 
# formatted_list <- gsub("_", " ", buildings)
# formatted_list <- gsub("^([a-z])", "\\U\\1", formatted_list, perl = TRUE)
# formatted_list_2 <- paste0('"', formatted_list, '"')
# 
# shops <- c("alcohol", "bakery", "beverages", "coffee", "convenience", "deli",
#                    "department_store", "general", "supermarket", "clothes", "gift")
# 
# formatted_list <- gsub("_", " ", shops)
# formatted_list <- gsub("^([a-z])", "\\U\\1", formatted_list, perl = TRUE)
# formatted_list_3 <- paste0('"', formatted_list, '"')
# 
# # Create a dataframe
# poi_types <- data.frame(Amenities = formatted_list_1, Buildings = formatted_list_2, Shops = formatted_list_3)
```

Assuming that distance to some certain types of buildings relates to how much cars and trucks need to use loading zones there, we calculated the distance from each pilot loading zone to the three nearest of each of these types.

```{r nearest neighbors, message=FALSE}
test <- st_read("./zones_nn_3.geojson")

numeric <- test %>%
  dplyr::select(-c("SmartZoneName")) %>%
  st_drop_geometry(.) %>%
  mutate(log10events = log10(events))

#Correlation plot
corplot <- cor(numeric)

cor_df <- as.data.frame(corplot)

# Extract the column values and row names
column_name <- "log10events"  # Replace "mpg" with the column name you want to plot
column_values <- cor_df[[column_name]]
cor_df <- data.frame(variable = rownames(cor_df), correlation = column_values)

cor_df <- filter(cor_df, variable != "events" & variable != "log10events")

# Create a bar plot using ggplot2
ggplot(cor_df, aes(x = reorder(variable, -correlation), y = correlation, fill=correlation)) +
  geom_bar(stat = "identity") +
  labs(title = "Relationships with Log10 Events",
       x = "Nearest Neighbor Variables",
       y = "Correlation") +
  ylim(-1,1)+
  scale_fill_viridis_c()+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1))
ggplotly(p=ggplot2::last_plot())
```

```{r clear NN table}
filter(cor_df, abs(correlation) > 0.5) %>%
  ggplot(., aes(x = reorder(variable, -correlation), y = correlation, fill=correlation)) +
  geom_bar(stat = "identity") +
  labs(title = "Strongest Relationships (>0.5) with Log10 Events",
       x = "Nearest Neighbor Variables",
       y = "Correlation") +
  ylim(-1,1)+
  scale_fill_viridis_c()+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1))
```

Many of the strongest relationships are negative ones. In particular, decreasing correlation with the number of parking events at a loading zone come with distance from a place of worship.

The positive relationships are associated with clinics and community centers.


### Open Mobility Foundation Data Exploration


```{r omf, eval=FALSE, include=FALSE}


```

### Next Steps

1. Make a predictive model for new zone expansion:
Predict segments of streets for the new loading zone.
Define the dependent variable as Booking by time by length of curb.

2. Articulate the rationale for putting different zones:
Analyze factors such as traffic flow, existing loading zones, parking demand, and pedestrian activity.
Consider input from stakeholders, including businesses, residents, and transportation authorities.

3. Use scenario planning:
Develop scenarios for placing loading zones in different locations within the project area.
Test each scenario to evaluate its impact on traffic flow, parking availability, and pedestrian safety.

## 3 Feature Engineering
```{r final}
final <- event_export %>%
  group_by(curb_zone_id,week,dotw) %>%
  summarise(Total_Events = n(),
            Total_Dwell_Time = sum(dwell_time_minutes),
            Average_Dwell_Time = mean(dwell_time_minutes))

week_dummies <- event_export %>%
  select(curb_zone_id, week, dotw,time_of_day) %>%
  group_by(curb_zone_id, week, dotw,time_of_day) %>%
  summarise(n = n(), .groups = 'drop') %>%
  spread(time_of_day, n, fill = 0)  

final <- week_dummies%>%
  left_join(final, by = c("curb_zone_id", "week","dotw"))
```

```{r join_geometry_amenities}
final_combined <- final %>%
  left_join(curb_geom[, c("OMFCurbZoneID","SmartZoneName", "geometry")], by = c("curb_zone_id" = "OMFCurbZoneID"))

test_selected <- test %>%
  select(-events) 

final_panel <- final_combined %>%
  left_join(test_selected, by = "SmartZoneName")
```

```{r import_weather, message = FALSE, warning = FALSE }

weather.Panel <- 
  riem_measures(station = "PHL", date_start = "2022-10-01", date_end = "2023-04-30") %>%
  dplyr::select(valid, tmpf, p01i, sknt) %>%
  replace(is.na(.), 0) %>%
  mutate(interval60 = as.POSIXct(valid, format="%Y-%m-%d %H:%M:%S", tz="UTC")) %>%
  mutate(week = week(interval60),
         year = year(interval60),
         dotw = wday(interval60, label=TRUE)) %>%
  group_by(year, week,dotw) %>%
  summarize(Precipitation = sum(p01i, na.rm = TRUE), .groups = 'drop')

final_panel <- final_panel %>%
  left_join(weather.Panel[, c("week", "dotw","Precipitation")], by = c("week","dotw"))

add_panel <- final_panel%>%
  select(SmartZoneName,curb_zone_id,week,dotw,AM_Rush,Mid_Day,overnight,PM_Rush,Total_Events,Precipitation,geometry.x) %>%
  rename(day=dotw,geometry=geometry.x)

```

```{r finalpanel, eval=FALSE, include=FALSE}
#st_write(final_panel, "./finalpanel.geojson")
st_write(add_panel, "./add.geojson")
```

```{r}
# Subset the dataframe to include only numeric columns
numeric_columns <- sapply(final_panel, is.numeric)
dat_num <- final_panel[, numeric_columns] %>% st_drop_geometry()
dat_nn <- dat_num %>%na.omit()


model_info <- data.frame(Variable = character(),
                         Estimate = numeric(),
                         StdError = numeric(),
                         tValue = numeric(),
                         pValue = numeric(),
                         stringsAsFactors = FALSE)

# Loop through each numeric variable, excluding the target variable
for(var in setdiff(names(dat_num), "Total_Events")) {
  formula <- as.formula(paste("Total_Events ~", var))
  model <- lm(formula, data = dat_num)
  summary_model <- summary(model)
  
  # Extracting key information
  model_info <- rbind(model_info, data.frame(Variable = var,
                                             Estimate = summary_model$coefficients[2, 1],
                                             StdError = summary_model$coefficients[2, 2],
                                             tValue = summary_model$coefficients[2, 3],
                                             pValue = summary_model$coefficients[2, 4]))
}

```

```{r}
model_info_sub <- final_panel%>% select(car,PM_Rush,overnight,Total_Dwell_Time,Sat,Wed,Thu,Fri,Tue,Mon,Sun,nn1_conference_centre_amenity,nn2_conference_centre_amenity,nn3_conference_centre_amenity,nn3_department_store_shop,nn2_clinic_amenity,nn2_department_store_shop,nn3_place_of_worship_amenity,nn2_place_of_worship_amenity,nn1_place_of_worship_amenity,nn1_department_store_shop,nn1_clinic_amenity,nn1_church_building,nn2_church_building,nn3_church_building,nn2_beverages_shop,nn2_clothes_shop,Total_Events)


ggcorrplot(
  round(cor(model_info_sub %>% st_drop_geometry()), 1), 
  p.mat = cor_pmat(model_info_sub), # Changed df to model_info_sub
  colors = c("#E63946", "white", "#2A9D8F"),
  type = "lower",
  insig = "blank",
  lab = TRUE
) + labs(title = 'Correlation across selected vars')


```

## 4 Predictive Model

### Data Preparation
```{r setting working directory, include=FALSE}
simple_panel <- read.csv("./03_31_panel.csv")
detail_panel <- read.csv("./detailed_03_31_panel.csv")
panel <- read.csv("./updated_04_01_panel.csv")
geo <- read.csv("./geometry.csv")
try <- read_sf("./finalpanel.geojson")
add <- read_sf("./add.geojson")
road_class <- read_sf("./bookings_road_class_nn.geojson")
```

```{r include=FALSE}
st_geometry(road_class) <- NULL
panel <- merge(panel, road_class, by = 'SmartZoneName', all.x = TRUE) %>%
  select(-Chestnut_St,-Walnut_St,-Sansom_St,-Broad_St,-east_bound,-west_bound,-two_way_north_south,-Bike_Network_dummy)%>%
  rename(day=Day,Road_Class=CLASS_2)

add <- add %>%
  rename(bookings=Total_Events)
  
st_geometry(add) <- NULL

panel2 <- panel%>%
  left_join(add,by=c("SmartZoneName","curb_zone_id","week","day","bookings"))

set.seed(123)

train_index_simple <- sample(1:nrow(panel), nrow(panel) * 0.7)

train_set_simple <- panel[train_index_simple, ]
test_set_simple <- panel[-train_index_simple, ]
```

### Data Modeling
Simplified Panel with time, road network, and nearest neighbors variables.

#### Hyper Parameters
mtry=50, ntree = 700, nodesize = 5,maxnodes = 20

```{r include=FALSE}
trControl <- trainControl(method = "cv",
    number = 5,
    search = "grid")

set.seed(2024)
tuneGrid <- expand.grid(.mtry = seq(30, 70, by = 5))


rf_mtry_2 <- train(bookings~.,
    data = train_set_simple,
    method = "rf",        # names(getModelInfo())
    metric = "RMSE", 
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    ntree = 100)

print(rf_mtry_2)

# best mtry=30
best_mtry_2 <- rf_mtry_2$bestTune$mtry

```

```{r eval=FALSE, include=FALSE}
# maxnodes
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = best_mtry_2)
for (maxnodes in c(2: 20)) {
    set.seed(2024)
    rf_maxnode <- train(bookings~.,
        data = train_set_simple,
        method = "rf",
        metric = "RMSE",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        maxnodes = maxnodes,
        ntree = 300)
    current_iteration <- toString(maxnodes)
    store_maxnode[[current_iteration]] <- rf_maxnode
}
# best maxnodes=20
results_maxnode <- resamples(store_maxnode)
summary(results_maxnode)
```

```{r eval=FALSE, include=FALSE}
# nodesize, regression: 5
store_nodesize <- list()
tuneGrid <- expand.grid(.mtry = best_mtry_2)
for (nodesize in c(2: 20)) {
    set.seed(2024)
    rf_nodesize <- train(bookings~.,
        data = train_set_simple,
        method = "rf",
        metric = "RMSE",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        maxnodes = 20,
        nodesize = nodesize,
        ntree = 300)
    current_iteration <- toString(nodesize)
    store_nodesize[[current_iteration]] <- rf_nodesize
}
# best nodesize=20
results_nodesize <- resamples(store_nodesize)
summary(results_nodesize)
```

```{r eval=FALSE, include=FALSE}
# ntree:500 default
store_maxtrees <- list()
for (ntree in c(100, 250, 300, 350, 400, 450, 500, 550, 600, 800, 1000, 2000, 3000)) {
    set.seed(2024)
    rf_maxtrees <- train(bookings~.,
        data = train_set_simple,
        method = "rf",
        metric = "RMSE",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        maxnodes = 20,
        ntree = ntree)
    key <- toString(ntree)
    store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
# ？
summary(results_tree)
```

#### Random Forest Model
```{r train_test, echo=TRUE}
train_set_simple<- train_set_simple%>%
  select(-SmartZoneName,-curb_zone_id,-week,-day,-geometry)
tuneGrid <- expand.grid(.mtry = best_mtry_2)
fit_rf_simple <- train(bookings~.,
    train_set_simple,
    method = "rf",
    metric = "RMSE",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    ntree = 700,
    nodesize = 5,
    maxnodes = 20)

varImp(fit_rf_simple)

prediction <-predict(fit_rf_simple, test_set_simple)

```

The chart visualizes the relative importance of the top 20 variables in a predictive model. Notably, the 'Weekend_dummy' variable stands out as the most significant predictor, followed by the distance to class 4 and class 1 roads, indicating their substantial influence on the model's output. Other proximity-related variables such as dining, housing, parking, retail, quick grocery, civic amenities, and schools are also identified as key factors, though to a lesser extent than weekend and road class variables.

```{r echo=TRUE, fig.height=4, fig.width=6}
top_n <- 20

ImpData <- as.data.frame(varImp(fit_rf_simple)$importance)
ImpData$Var.Names <- row.names(ImpData)

# Sort by importance, and take the top 'top_n' most important variables
ImpDataTop <- ImpData %>%
  top_n(top_n, Overall) %>%
  arrange(desc(Overall))

# Use the filtered data frame to create a chart
ggplot(ImpDataTop, aes(x=Var.Names, y=Overall)) +
  geom_segment(aes(x=Var.Names, xend=Var.Names, y=0, yend=Overall), color="skyblue") +
  geom_point(aes(y=Overall), color="blue", alpha=0.6) +  # Plot points using the Overall column
  theme_light() +
  coord_flip() +  # Display variable names on the y-axis
  theme(
    legend.position="none",  # Remove legend
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(size = 10)  # Adjust font size
  ) +
  labs(x = "Variable Name", y = "Importance", title = "Top Variable Importance")

```

```{r}
# Assuming 'prediction' contains your model's predictions and 'test_set' is your test dataset
# Add a residuals column to test_set
test_set_simple <- test_set_simple %>%
  mutate(
    Prediction = prediction,
    Residual = bookings - Prediction,
    AbsResidual = abs(Residual)
  )
#write_csv(test_set_simple,"./testresult.csv")
```

### Model Generalizability Evaluation

#### Examine Error Metrics for Accuracy

The provided chart showcases the performance of a Random Forest model used for predicting bookings on a weekly basis. It indicates that the model tends to underperform particularly in extreme scenarios, such as when bookings are unusually high or low. The model shows a noticeable disparity in predictive accuracy for bookings that exceed an average of 5, often underestimating the actual values. Additionally, there's an observable seasonal trend in bookings, which the model does not seem to fully capture. This limitation is likely due to the model being trained on a dataset limited to the months of October through April, which may not be representative of the entire year. This seasonal effect and the limited data scope could be contributing to the model's reduced effectiveness during peak booking periods in summer.

```{r echo=TRUE, fig.height=6, fig.width=12}

# add a 'year' variable to distinguish between years, then create an 'ordered_week' variable for proper week ordering.
test_set_simple <- test_set_simple %>%
  mutate(year = if_else(week >= 42, "2022", "2023")) %>%  # Assign "Year1" or "Year2" based on the week number
  arrange(year, week) %>%  # Sort by year and week
  mutate(ordered_week = row_number())  # Assign a unique ordering number for each week

# Now use this 'ordered_week' variable to draw your plot
test_set_simple %>%
  select(SmartZoneName, ordered_week, day, bookings, Prediction) %>%
  gather(Variable, Value, bookings, Prediction) %>%  # Transform data to long format for plotting
  group_by(Variable, ordered_week, day) %>%
  summarize(Value = mean(Value, na.rm = TRUE), .groups = 'drop') %>%  # Calculate average value for each group
  ggplot(aes(x = ordered_week, y = Value, color = Variable, group = Variable)) +  # Plotting
    geom_line(size = 0.7) + 
  scale_color_manual(values = c("bookings" = "#440154", "Prediction" = "#35B779")) +
    scale_x_continuous(breaks = test_set_simple$ordered_week, labels = test_set_simple$week) +  # Custom x-axis breaks and labels
    labs(title = "Predicted/Observed Bookings", 
         subtitle = "Random Forest Model by Week", 
         x = "Week of Year", 
         y = "Average Bookings") +  # Labels and titles
    theme_minimal() +  # Minimal theme
    theme(axis.text.x = element_blank()  # Hide x-axis text
    )
    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5))  # Adjust the appearance of x-axis text

```

#### Space-Time Error Evaluation
```{r}
error_analysis_curb <- test_set_simple %>%
  group_by(SmartZoneName,CLASS_2) %>%
  summarise(
    MeanResidual = mean(Residual, na.rm = TRUE),
    MeanAbsResidual = mean(AbsResidual, na.rm = TRUE),
    MSE = mean(Residual^2, na.rm = TRUE),
    Count = n()
  ) %>%
  arrange(desc(MeanAbsResidual))
```

This map provides a spatial overview of the model’s performance across different streets, where larger circles indicate a higher MAE at that curb, suggesting that the model's predictions are less accurate on Chestnut St, while Walnut St has a better performance.

```{r echo=TRUE, fig.height=4, fig.width=8}
# Now group by SmartZoneName and summarize the residuals

error_analysis_curb <- test_set_simple %>%
  group_by(SmartZoneName, Road_Class) %>%
  summarise(
    MeanResidual = mean(Residual, na.rm = TRUE),  # 平均误差
    MAE = mean(AbsResidual, na.rm = TRUE),  # 平均绝对误差（MAE）
    MSE = mean(Residual^2, na.rm = TRUE),  # 均方误差
    RMSE = sqrt(MSE),  # 均方根误差
    Count = n(),  # 计数
    .groups = "drop"  # 防止之后版本的 dplyr 报错
  ) %>%
  arrange(desc(MAE))

# View the error analysis results
print(error_analysis_curb)

geo <- geo %>%
  select(SmartZoneName,geometry)%>%
  distinct(SmartZoneName, geometry)

geo_points <- geo$geometry %>%
  stringr::str_extract_all("[-]?[0-9]+\\.[0-9]+") %>% # 提取所有数字（包括小数点和负号）
  purrr::map(~st_point(as.numeric(.x), dim = "XY")) # 将每个提取出来的数值向量转换为POINT对象

geo_sf <- st_as_sf(geo, geometry = do.call("st_sfc", geo_points), crs = 4326) %>% # crs 4326是WGS84坐标系
  select(-geometry)

error_analysis_curb <- geo_sf %>%
  left_join(error_analysis_curb, by=c("SmartZoneName"))

# Set tmap mode to view for interactive maps
tmap_mode("view")

tm <- tm_shape(error_analysis_curb) +
  tm_bubbles(size = "MAE", col = "MAE", palette = "viridis", scale=5, border.col = "black", 
             title.size = "MAE", title.col = "MAE") 
tm
```


From the chart, the high error in conjunction with high data counts suggest a systematic issue with the model especially on Monday, rather than a random fluctuation. As such, there's need to further check the weekly patterns or specific events impacting the model performance. While those low-low points, like Sunday, demonstrate its good performance on weekends.

```{r echo=TRUE, fig.height=4, fig.width=6}
# Now group by SmartZoneName and summarize the residuals
error_analysis_day <- test_set_simple %>%
  group_by(day) %>%
  summarise(
    MeanResidual = mean(Residual, na.rm = TRUE),
    MAE = mean(AbsResidual, na.rm = TRUE),
    MSE = mean(Residual^2, na.rm = TRUE),
    Count = n()
  ) %>%
  arrange(desc(MAE))

# View the error analysis results
print(error_analysis_day)

error_analysis_day$day <- factor(error_analysis_day$day, 
                                 levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"), 
                                 ordered = TRUE)


ggplot(error_analysis_day, aes(x = day, y = MAE)) +
  geom_point(aes(size = Count, color = MAE)) +  
  scale_color_gradient(low = "#35B779", high = "#440154") +  
  labs(x = "Day of the Week", 
       y = "Mean Absolute Error (MAE)", 
       title = "Daily Mean Absolute Error") +
  theme_minimal() +
  theme(legend.position = "bottom") 

```

```{r echo=TRUE, fig.height=2, fig.width=4}
# Now group by SmartZoneName and summarize the residuals
error_analysis_weekend <- test_set_simple %>%
  group_by(Weekend_dummy) %>%
  summarise(
    MeanResidual = mean(Residual, na.rm = TRUE),
    MAE = mean(AbsResidual, na.rm = TRUE),
    MSE = mean(Residual^2, na.rm = TRUE),
    Count = n()
  ) %>%
  arrange(desc(MAE))

# View the error analysis results
print(error_analysis_weekend)

ggplot(error_analysis_weekend, aes(x = factor(Weekend_dummy), y = MAE)) +
  geom_bar(stat = "identity", aes(fill = factor(Weekend_dummy))) +
  scale_fill_manual(values = c("#35B779", "#440154")) + 
  labs(x = "Weekend (1) or Not (0)", y = "Mean Absolute Residual", 
       title = "Mean Absolute Residuals by Weekend or Weekday") +
  theme_minimal()

```

Overall, the model seems to be more accurate and precise for lower values of bookings and shows some divergence from the observed data at higher booking values. The similarity of the trend lines in both panels suggests the model performs consistently across weekdays and weekends.

```{r echo=TRUE, fig.height=2, fig.width=8}
ggplot()+
  geom_point(data = test_set_simple,aes(x= bookings, y = prediction),color = "#35B779")+
    geom_smooth(data = test_set_simple,aes(x= bookings, y= prediction), method = "lm", se = FALSE, color = '#440154')+
    geom_abline(slope = 1, intercept = 0)+
  facet_grid(~Weekend_dummy)+
  labs(title="Observed vs Predicted",
       subtitle = 'weekday(0) vs weekend(1) camparison',
       x="Bookings", 
       y="Predicted Bookings")+
  plotTheme()+
  theme_minimal()
```

#### Cross-validation

The cross-validation chart indicates that for a random forest model subjected to a 50-fold cross-validation process, the Goodness of Fit metrics—MAE, RMSE, and Rsquared—are closely clustered around their respective means. Specifically, the MAE above 1 and RMSE values suggest a consistent average error magnitude across folds. The Rsquared values around 0.5 imply that the model explains about half of the variability in the data. Those tight clustering of these metrics further suggests that the model has stable performance and reasonable generalizability across different data subsets.

```{r cv, echo=TRUE, fig.height=4, fig.width=8}
fitControl <- trainControl(method = "cv", number = 50)
set.seed(825)

rf.cv <- train(bookings ~ ., data = train_set_simple, 
               method = "rf",
               trControl = fitControl,
               tuneGrid = expand.grid(.mtry = best_mtry_2),
               importance = TRUE,
               ntree = 700,
               nodesize = 5,
               maxnodes = 20)

rf.cv

dplyr::select(rf.cv$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(rf.cv$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) + 
    geom_histogram(bins=35, fill = "#2a9d8f") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#e76f51", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 2)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics",
         subtitle = "50 folds, Across-fold mean reprented as dotted lines") +
    plotTheme()

```

## 5 Prediction on all curbs

```{r}
all_curbs <- read_sf("./all_curbs_0415.geojson")
all_curbs_nn <- read_sf("./all_curbs_nn.geojson")

all_curbs_weekday <- all_curbs %>%
  select(-TARGET_FID,-end_st_.x,-strt_s_.x,-strt_nm.x,-OID_,-Join_Count,-categry,-curb_id,-currncy,-dstnc_n_,-dstnc_s_,-end_st_.y,-mx_drt_,-price,-rgltn_t,-sd_f_st,-smrt_z_,-strt_s_.y,-strt_nm.y,-time_zn,-vhcl_ty,-Shape_Length,-geometry) %>%
  mutate(Weekend_dummy=0) 

all_curbs_weekend <- all_curbs %>%
  select(-TARGET_FID,-end_st_.x,-strt_s_.x,-strt_nm.x,-OID_,-Join_Count,-categry,-curb_id,-currncy,-dstnc_n_,-dstnc_s_,-end_st_.y,-mx_drt_,-price,-rgltn_t,-sd_f_st,-smrt_z_,-strt_s_.y,-strt_nm.y,-time_zn,-vhcl_ty,-Shape_Length,-geometry) %>%
  mutate(Weekend_dummy=1)

allPredictions_weekday <- 
  predict(fit_rf_simple, all_curbs_weekday)
  
all_curbs_weekday <- 
  cbind(all_curbs_weekday, allPredictions_weekday)

allPredictions_weekend <- 
  predict(fit_rf_simple, all_curbs_weekend)
allPredictions_weekend <- 
  predict(fit_rf_simple, all_curbs_weekend)
all_curbs_weekend <- 
  cbind(all_curbs_weekend, allPredictions_weekend)
```

```{r}
ggplot() + 
  geom_sf(data = all_curbs_weekday, 
          aes(fill = allPredictions_weekday), 
          color = "#e76f51") +
  scale_fill_gradientn(colors = c("#F7FBFF", "#bcc47a"),
                       name = "Predicted Bookings") +
  labs(title = "Curb Demand Prediction in Piladelphia, PA") +
  theme_minimal()
```

